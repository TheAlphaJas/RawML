{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with rawML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import some basic methods and functions.\n",
    "\n",
    "The core rawML library contains jTensors (rML equivalent of torch tensors), createModel method (which is used to stich together different components), and other essentials.\n",
    "\n",
    "rawML.layers contains all defined layers (relu, linear etc)\n",
    "\n",
    "rawML.optimzers contains all available optimizers for model parameter tuning\n",
    "\n",
    "rawML.losses contains all loss functions.\n",
    "\n",
    "These have been created because rawML losses/optimizers require certain methods to be present to work with the overall flow. A guide on custom layers/optimizers/loss functions shall be released later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to rawML\n"
     ]
    }
   ],
   "source": [
    "import rawML as rML #Core library \n",
    "#The first import will return a small greeting :)\n",
    "\n",
    "#Here we define a jTensor. \n",
    "tensor1 = rML.jTensor([1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jTensors inherit from numpy.ndarrays, with the primary difference being that they also consist of an attribute (.gd) [strikingly similar to torch.grad], which contains the grads of that particular tensor. \n",
    "\n",
    "This is useful during backpropogation and parameters update.\n",
    "\n",
    "rML supports basic mathematical methods like mean, min, std etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rawML.backend.jTensor'>\n",
      "(3,)\n",
      "2.0\n",
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(type(tensor1))\n",
    "print(tensor1.shape)\n",
    "print(rML.mean(tensor1))\n",
    "print(rML.min(tensor1))\n",
    "print(rML.std(tensor1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a rML model, one needs to first define a list of model layers. <br>\n",
    "Layers can be imported from rawML.layers. We shall take linear and relu for our small example <br>\n",
    "To create an instance of a Linear layer, one needs to specify the input dimensions and the output dimensions\n",
    "By default, all linear layers are initialized using He normal initialization. <br>\n",
    "All layers have the .get_wb() and .show_wb() method, which return and show the weight and bias jTensors corresponding to that layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rawML.layers.linear'>\n",
      "Weight Matrix :-\n",
      "[[ 0.48748231  1.47880723  0.42344587 -1.49559028]\n",
      " [ 0.79058535 -1.3254287  -0.76368347  0.71308381]\n",
      " [-0.09159816  1.15006461 -0.32920608 -0.34791512]]\n",
      "Bias Matrix :-\n",
      "[[0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from rawML.layers import relu, linear\n",
    "L1 = linear(3,4)\n",
    "print(type(L1))\n",
    "L1.show_wb()\n",
    "\n",
    "layers_list = [\n",
    "    L1,\n",
    "    relu(),\n",
    "    linear(4,2)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining things required to define a rML model are the optimizer, and the loss function. Both can be imported from the respective libraries.\n",
    "\n",
    "For the Gradient Descent Optimizer, one can get and vary the lr using .get_lr() and .update_lr() methods respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rawML.losses import MSELoss\n",
    "from rawML.optimizers import GDOptimizer\n",
    "\n",
    "loss = MSELoss()\n",
    "optim = GDOptimizer(5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine it all together!\n",
    "The core rML library comes with a createModel class, which takes input the layers list, optimizer and loss function.<br>\n",
    "Each layer and loss function have a backward method, which computes the dLoss/d(tensor) corresponding to that particular layer. <br> \n",
    "All forward passes, backward passes and parameters updating using optimzer have been wrapped in the model.train method. <br>\n",
    "Ofcouse, one can set the number of epochs and verbose frequency too!\n",
    "\n",
    "The model.train mandatorily takes a training split and a validation split. <br>\n",
    "It has to be passed as (x_train,y_train),(x_val,y_val), as show below.\n",
    "\n",
    "rML.randn([shape]), and rML.rand([shape]) can be used to create random arrays of the specified dimensions.\n",
    "\n",
    "Also note that for this demo, we use sklearn's test_train_split to create us a random split.[Yes, they support jTensors] Although the data is random, the purpose of this demo is to give an idea on how rML works, and how to use it as a high level API for model development, training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rML.createModel(layers_list, optim, loss)\n",
    "\n",
    "#Creating Random X,Y and split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = rML.randn([20,3])\n",
    "Y = rML.rand([20,2])\n",
    "X_train,X_val,Y_train,Y_val = tts(X,Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:1 | trainLoss: 0.14100, valLoss: 0.38986, optLr: 0.005\n",
      "e:6 | trainLoss: 0.07721, valLoss: 0.27050, optLr: 0.005\n",
      "e:11 | trainLoss: 0.05329, valLoss: 0.21080, optLr: 0.005\n",
      "e:16 | trainLoss: 0.04144, valLoss: 0.17568, optLr: 0.005\n",
      "e:21 | trainLoss: 0.03430, valLoss: 0.15307, optLr: 0.005\n",
      "e:26 | trainLoss: 0.02953, valLoss: 0.13762, optLr: 0.005\n",
      "e:31 | trainLoss: 0.02603, valLoss: 0.12657, optLr: 0.005\n",
      "e:36 | trainLoss: 0.02288, valLoss: 0.11751, optLr: 0.005\n",
      "e:41 | trainLoss: 0.02032, valLoss: 0.11068, optLr: 0.005\n",
      "e:46 | trainLoss: 0.01820, valLoss: 0.10539, optLr: 0.005\n"
     ]
    }
   ],
   "source": [
    "model.train((X_train,Y_train), (X_val,Y_val), epochs = 50, verbose_freq= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how we can access the weights, biases of individual layers. <br>\n",
    "Note that L1 was defined above, as the first linear layer\n",
    "\n",
    "One noted issue is that grads cannot be accessed outside train method. I am working on that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix :-\n",
      "[[ 0.48235223  1.45980461 -0.04983975 -1.38805826]\n",
      " [ 0.7908057  -1.31400947 -0.52531809  0.58301378]\n",
      " [-0.07586737  1.15576805 -0.17298106 -0.38553533]]\n",
      "Bias Matrix :-\n",
      "[[-1.90042869e-04 -1.11061402e-02 -2.64072810e-01 -1.77050586e-01]]\n"
     ]
    }
   ],
   "source": [
    "L1.show_wb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! We successfully trained a model using rawML.\n",
    "Inference is fairly straightforward, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "y_out = model(rML.randn([50,3]))\n",
    "print(y_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
