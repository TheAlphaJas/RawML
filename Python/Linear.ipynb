{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9ad5cf-a258-4d9e-b6b8-1fc52ea62770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2cbcfe24-dbc6-4364-9af8-292bd2113d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jTensor(np.ndarray):\n",
    "    def __new__(cls, ip_arr, gd=None):\n",
    "        obj = np.asarray(ip_arr).view(cls).astype(np.float64)\n",
    "        obj.gd = gd\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "a0738ddc-6092-441a-b606-2ecad6ccacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDOptimizer:\n",
    "    def __init__(self,lr):\n",
    "        self.lr = lr\n",
    "    def update(self,tup):\n",
    "        w,b = tup\n",
    "        if (w is None):\n",
    "            return None,None\n",
    "        return (w - self.lr*w.gd, b - self.lr*b.gd)\n",
    "    def get_lr(self,):\n",
    "        return self.lr\n",
    "    def update_lr(self,lr):\n",
    "        self.lr = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "faabc633-bb4e-446f-bd27-e2bc47a1c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, n_in, n_out, b):\n",
    "        self.batch_size = b\n",
    "        self.w = jTensor(np.random.randn(n_in, n_out)*sqrt(2.0/n_in)) #He initialization\n",
    "        self.b = jTensor(np.zeros((self.batch_size,n_out)))\n",
    "    def __call__(self,x):\n",
    "        return (np.dot(x,self.w) + self.b);\n",
    "    def get_wb(self,):\n",
    "        return (self.w,self.b)\n",
    "    def backprop(self,x_in,x_out):\n",
    "        x_in.gd = np.dot(x_out.gd,self.w.T)\n",
    "        self.w.gd = np.dot(x_in.T,x_out.gd)\n",
    "        self.b.gd = x_out.gd\n",
    "    def update_wb(self,tup):\n",
    "        w_new,b_new = tup\n",
    "        self.w = w_new\n",
    "        self.b = b_new\n",
    "    def show_wb(self,):\n",
    "        print(\"Weight Matrix\")\n",
    "        print(self.w)\n",
    "        print(\"Bias Matrix\")\n",
    "        print(self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "6cdb9468-ecec-4f39-8129-4203cdca2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    def __call__(self,x):\n",
    "        return np.clip(x,a_min=0.0, a_max = None)\n",
    "    def get_wb(self,):\n",
    "        return (None,None)\n",
    "    def backprop(self,x_in,x_out):\n",
    "        temp = x_in\n",
    "        temp[temp>0]=1.0\n",
    "        temp[temp<=0]=0.0\n",
    "        # dL/dx_in = dL/dxout * dxout/xin\n",
    "        x_in.gd = (x_in > 0).astype(np.float64)*x_out.gd\n",
    "    def update_wb(self,tup):\n",
    "        w_new, b_new = None,None\n",
    "    # def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "d0e3aacc-8385-481f-803e-1fe3ef810d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __call__(self,x,xt):\n",
    "        return ((x-xt)**2)/len(x)\n",
    "    def backprop(self,x,xt):\n",
    "        x.gd = (2*(x-xt))/len(x) #dLoss/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "28a1102b-4197-4790-9abe-e36e2dcabbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel:\n",
    "    def __init__(self, layers_list, optimizer, loss_fn):\n",
    "        self.layers_list = layers_list\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss_fn\n",
    "    def __call__(self,x_in):\n",
    "        x_out = x_in\n",
    "        for l in self.layers_list:\n",
    "            x_out = l(x_out)\n",
    "        return x_out\n",
    "    def train(self,x_in,y_true, epochs=1):\n",
    "        # assert len(x_in) == len(y_true) \"Input vector and Output vector batch size should be equal!\"\n",
    "        for i in range(epochs):\n",
    "            x_list=[]\n",
    "            x_list.append(x_in)\n",
    "            for l in self.layers_list:\n",
    "                y = l(x_list[-1])\n",
    "                # print(y.shape)\n",
    "                x_list.append(y)\n",
    "            print(\"Epoch: {}, Training Loss: {}, Optimizer lr: {}\".format(i+1,np.mean(self.loss(x_list[-1],y_true)),self.optimizer.get_lr()))\n",
    "            \n",
    "            #Backprop\n",
    "            self.loss.backprop(x_list[-1],y_true)\n",
    "            for i in range(len(self.layers_list)):\n",
    "                self.layers_list[-i-1].backprop(x_list[-i-2],x_list[-i-1])\n",
    "            #Update\n",
    "            for l in self.layers_list:\n",
    "                l.update_wb(self.optimizer.update(l.get_wb()))\n",
    "            x_list.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "c59a0168-4240-4516-8f4a-ec751ed6a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = [\n",
    "    LinearLayer(50,30,10),\n",
    "    ReluLayer(),\n",
    "    LinearLayer(30,40,10),\n",
    "]\n",
    "\n",
    "optim = GDOptimizer(lr = 1e-2)\n",
    "loss = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "7d45d668-e291-4da9-9a3e-0cd2c52f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreateModel(layers_list,optim,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "97badb52-e4fe-45f8-a507-3509ac9d13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = jTensor(np.random.rand(10,50))\n",
    "Y_true = jTensor(np.random.rand(10,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "cbf3d496-9760-43b0-af8d-0c96277554da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 40)\n",
      "<class '__main__.jTensor'>\n"
     ]
    }
   ],
   "source": [
    "#Testing forward pass\n",
    "y = model(X_in)\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "31618e94-d620-4387-820f-0577aa408171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.0021239223838360453, Optimizer lr: 0.01\n",
      "Epoch: 2, Training Loss: 0.0021118771829258753, Optimizer lr: 0.01\n",
      "Epoch: 3, Training Loss: 0.0020998290911941313, Optimizer lr: 0.01\n",
      "Epoch: 4, Training Loss: 0.0020879974371941657, Optimizer lr: 0.01\n",
      "Epoch: 5, Training Loss: 0.0020761337709716203, Optimizer lr: 0.01\n",
      "Epoch: 6, Training Loss: 0.00206448296253606, Optimizer lr: 0.01\n",
      "Epoch: 7, Training Loss: 0.0020524837955682, Optimizer lr: 0.01\n",
      "Epoch: 8, Training Loss: 0.0020408477549354216, Optimizer lr: 0.01\n",
      "Epoch: 9, Training Loss: 0.0020292198202974373, Optimizer lr: 0.01\n",
      "Epoch: 10, Training Loss: 0.0020179847724203785, Optimizer lr: 0.01\n",
      "Epoch: 11, Training Loss: 0.002006938109740224, Optimizer lr: 0.01\n",
      "Epoch: 12, Training Loss: 0.0019952457132848865, Optimizer lr: 0.01\n",
      "Epoch: 13, Training Loss: 0.001984077198078588, Optimizer lr: 0.01\n",
      "Epoch: 14, Training Loss: 0.001972677923135854, Optimizer lr: 0.01\n",
      "Epoch: 15, Training Loss: 0.0019612611763328443, Optimizer lr: 0.01\n",
      "Epoch: 16, Training Loss: 0.001949982219151848, Optimizer lr: 0.01\n",
      "Epoch: 17, Training Loss: 0.0019390861531357877, Optimizer lr: 0.01\n",
      "Epoch: 18, Training Loss: 0.0019279881145174448, Optimizer lr: 0.01\n",
      "Epoch: 19, Training Loss: 0.001916924437874524, Optimizer lr: 0.01\n",
      "Epoch: 20, Training Loss: 0.0019061421220972396, Optimizer lr: 0.01\n",
      "Epoch: 21, Training Loss: 0.0018954201311906205, Optimizer lr: 0.01\n",
      "Epoch: 22, Training Loss: 0.0018845189181747768, Optimizer lr: 0.01\n",
      "Epoch: 23, Training Loss: 0.0018737782321942694, Optimizer lr: 0.01\n",
      "Epoch: 24, Training Loss: 0.0018631735542175942, Optimizer lr: 0.01\n",
      "Epoch: 25, Training Loss: 0.0018529521198115495, Optimizer lr: 0.01\n",
      "Epoch: 26, Training Loss: 0.0018421702838998066, Optimizer lr: 0.01\n",
      "Epoch: 27, Training Loss: 0.0018316737715380672, Optimizer lr: 0.01\n",
      "Epoch: 28, Training Loss: 0.0018213175481214382, Optimizer lr: 0.01\n",
      "Epoch: 29, Training Loss: 0.0018110670743105616, Optimizer lr: 0.01\n",
      "Epoch: 30, Training Loss: 0.0018006672076727125, Optimizer lr: 0.01\n",
      "Epoch: 31, Training Loss: 0.0017903297176281856, Optimizer lr: 0.01\n",
      "Epoch: 32, Training Loss: 0.0017805510230795788, Optimizer lr: 0.01\n",
      "Epoch: 33, Training Loss: 0.0017710472417743629, Optimizer lr: 0.01\n",
      "Epoch: 34, Training Loss: 0.001760732203897473, Optimizer lr: 0.01\n",
      "Epoch: 35, Training Loss: 0.0017503726094997645, Optimizer lr: 0.01\n",
      "Epoch: 36, Training Loss: 0.0017403605582866034, Optimizer lr: 0.01\n",
      "Epoch: 37, Training Loss: 0.0017306021462165063, Optimizer lr: 0.01\n",
      "Epoch: 38, Training Loss: 0.0017205758730447734, Optimizer lr: 0.01\n",
      "Epoch: 39, Training Loss: 0.0017107129791397327, Optimizer lr: 0.01\n",
      "Epoch: 40, Training Loss: 0.0017012079720368195, Optimizer lr: 0.01\n",
      "Epoch: 41, Training Loss: 0.0016915542065881037, Optimizer lr: 0.01\n",
      "Epoch: 42, Training Loss: 0.0016818788121498653, Optimizer lr: 0.01\n",
      "Epoch: 43, Training Loss: 0.001672348712202702, Optimizer lr: 0.01\n",
      "Epoch: 44, Training Loss: 0.0016630556403221647, Optimizer lr: 0.01\n",
      "Epoch: 45, Training Loss: 0.0016536818878465076, Optimizer lr: 0.01\n",
      "Epoch: 46, Training Loss: 0.0016440987287812132, Optimizer lr: 0.01\n",
      "Epoch: 47, Training Loss: 0.001634831134846562, Optimizer lr: 0.01\n",
      "Epoch: 48, Training Loss: 0.0016261208612871258, Optimizer lr: 0.01\n",
      "Epoch: 49, Training Loss: 0.0016166836498260966, Optimizer lr: 0.01\n",
      "Epoch: 50, Training Loss: 0.001607247567622207, Optimizer lr: 0.01\n",
      "Epoch: 51, Training Loss: 0.001597977789814966, Optimizer lr: 0.01\n",
      "Epoch: 52, Training Loss: 0.0015890795305038218, Optimizer lr: 0.01\n",
      "Epoch: 53, Training Loss: 0.001580474493384642, Optimizer lr: 0.01\n",
      "Epoch: 54, Training Loss: 0.001571213458250678, Optimizer lr: 0.01\n",
      "Epoch: 55, Training Loss: 0.0015622779951905625, Optimizer lr: 0.01\n",
      "Epoch: 56, Training Loss: 0.0015534889748851533, Optimizer lr: 0.01\n",
      "Epoch: 57, Training Loss: 0.0015446001589966868, Optimizer lr: 0.01\n",
      "Epoch: 58, Training Loss: 0.00153576171737262, Optimizer lr: 0.01\n",
      "Epoch: 59, Training Loss: 0.0015270733064673282, Optimizer lr: 0.01\n",
      "Epoch: 60, Training Loss: 0.0015186095865114286, Optimizer lr: 0.01\n",
      "Epoch: 61, Training Loss: 0.0015098384784509888, Optimizer lr: 0.01\n",
      "Epoch: 62, Training Loss: 0.001501369476671903, Optimizer lr: 0.01\n",
      "Epoch: 63, Training Loss: 0.0014930833514460045, Optimizer lr: 0.01\n",
      "Epoch: 64, Training Loss: 0.0014846617582727811, Optimizer lr: 0.01\n",
      "Epoch: 65, Training Loss: 0.001476152418072338, Optimizer lr: 0.01\n",
      "Epoch: 66, Training Loss: 0.0014676611466044888, Optimizer lr: 0.01\n",
      "Epoch: 67, Training Loss: 0.0014595196537863597, Optimizer lr: 0.01\n",
      "Epoch: 68, Training Loss: 0.0014517914468879988, Optimizer lr: 0.01\n",
      "Epoch: 69, Training Loss: 0.0014431769971780895, Optimizer lr: 0.01\n",
      "Epoch: 70, Training Loss: 0.0014348301338074454, Optimizer lr: 0.01\n",
      "Epoch: 71, Training Loss: 0.0014266926290741084, Optimizer lr: 0.01\n",
      "Epoch: 72, Training Loss: 0.0014187543522894757, Optimizer lr: 0.01\n",
      "Epoch: 73, Training Loss: 0.0014106371485778527, Optimizer lr: 0.01\n",
      "Epoch: 74, Training Loss: 0.001402557675091558, Optimizer lr: 0.01\n",
      "Epoch: 75, Training Loss: 0.0013947553608975691, Optimizer lr: 0.01\n",
      "Epoch: 76, Training Loss: 0.0013869755495349938, Optimizer lr: 0.01\n",
      "Epoch: 77, Training Loss: 0.0013789779226290404, Optimizer lr: 0.01\n",
      "Epoch: 78, Training Loss: 0.001371209431990551, Optimizer lr: 0.01\n",
      "Epoch: 79, Training Loss: 0.0013635029881536446, Optimizer lr: 0.01\n",
      "Epoch: 80, Training Loss: 0.0013561778983457934, Optimizer lr: 0.01\n",
      "Epoch: 81, Training Loss: 0.001348311500206303, Optimizer lr: 0.01\n",
      "Epoch: 82, Training Loss: 0.0013405731814197532, Optimizer lr: 0.01\n",
      "Epoch: 83, Training Loss: 0.0013331192024818679, Optimizer lr: 0.01\n",
      "Epoch: 84, Training Loss: 0.0013255832061017413, Optimizer lr: 0.01\n",
      "Epoch: 85, Training Loss: 0.0013181582342516528, Optimizer lr: 0.01\n",
      "Epoch: 86, Training Loss: 0.0013110217073392212, Optimizer lr: 0.01\n",
      "Epoch: 87, Training Loss: 0.0013035416550656598, Optimizer lr: 0.01\n",
      "Epoch: 88, Training Loss: 0.0012961389438403135, Optimizer lr: 0.01\n",
      "Epoch: 89, Training Loss: 0.001288666846055718, Optimizer lr: 0.01\n",
      "Epoch: 90, Training Loss: 0.001281331346491623, Optimizer lr: 0.01\n",
      "Epoch: 91, Training Loss: 0.0012741962388908574, Optimizer lr: 0.01\n",
      "Epoch: 92, Training Loss: 0.0012670559659538436, Optimizer lr: 0.01\n",
      "Epoch: 93, Training Loss: 0.0012598257115831078, Optimizer lr: 0.01\n",
      "Epoch: 94, Training Loss: 0.0012527250920992084, Optimizer lr: 0.01\n",
      "Epoch: 95, Training Loss: 0.0012456802696078836, Optimizer lr: 0.01\n",
      "Epoch: 96, Training Loss: 0.001238802310215478, Optimizer lr: 0.01\n",
      "Epoch: 97, Training Loss: 0.0012317234799663873, Optimizer lr: 0.01\n",
      "Epoch: 98, Training Loss: 0.0012247185208890737, Optimizer lr: 0.01\n",
      "Epoch: 99, Training Loss: 0.0012179121258156607, Optimizer lr: 0.01\n",
      "Epoch: 100, Training Loss: 0.0012115766009611242, Optimizer lr: 0.01\n",
      "Epoch: 101, Training Loss: 0.0012044503349030137, Optimizer lr: 0.01\n",
      "Epoch: 102, Training Loss: 0.001197542268752109, Optimizer lr: 0.01\n",
      "Epoch: 103, Training Loss: 0.0011907683548668109, Optimizer lr: 0.01\n",
      "Epoch: 104, Training Loss: 0.0011841777897792687, Optimizer lr: 0.01\n",
      "Epoch: 105, Training Loss: 0.001177499044518814, Optimizer lr: 0.01\n",
      "Epoch: 106, Training Loss: 0.0011708209186381438, Optimizer lr: 0.01\n",
      "Epoch: 107, Training Loss: 0.0011642793348605326, Optimizer lr: 0.01\n",
      "Epoch: 108, Training Loss: 0.0011578772724105872, Optimizer lr: 0.01\n",
      "Epoch: 109, Training Loss: 0.0011513551027537853, Optimizer lr: 0.01\n",
      "Epoch: 110, Training Loss: 0.0011448701127699252, Optimizer lr: 0.01\n",
      "Epoch: 111, Training Loss: 0.0011384565263617922, Optimizer lr: 0.01\n",
      "Epoch: 112, Training Loss: 0.0011323340646366609, Optimizer lr: 0.01\n",
      "Epoch: 113, Training Loss: 0.0011258645409382712, Optimizer lr: 0.01\n",
      "Epoch: 114, Training Loss: 0.0011194711592625375, Optimizer lr: 0.01\n",
      "Epoch: 115, Training Loss: 0.0011131473359571743, Optimizer lr: 0.01\n",
      "Epoch: 116, Training Loss: 0.0011070525854641538, Optimizer lr: 0.01\n",
      "Epoch: 117, Training Loss: 0.001100719651136304, Optimizer lr: 0.01\n",
      "Epoch: 118, Training Loss: 0.0010945381259269862, Optimizer lr: 0.01\n",
      "Epoch: 119, Training Loss: 0.0010884205390258943, Optimizer lr: 0.01\n",
      "Epoch: 120, Training Loss: 0.0010824816456629007, Optimizer lr: 0.01\n",
      "Epoch: 121, Training Loss: 0.0010763830066245327, Optimizer lr: 0.01\n",
      "Epoch: 122, Training Loss: 0.0010706343480912624, Optimizer lr: 0.01\n",
      "Epoch: 123, Training Loss: 0.001064444814081007, Optimizer lr: 0.01\n",
      "Epoch: 124, Training Loss: 0.0010584261656878633, Optimizer lr: 0.01\n",
      "Epoch: 125, Training Loss: 0.001052583025498064, Optimizer lr: 0.01\n",
      "Epoch: 126, Training Loss: 0.0010465863643154849, Optimizer lr: 0.01\n",
      "Epoch: 127, Training Loss: 0.0010407559308343705, Optimizer lr: 0.01\n",
      "Epoch: 128, Training Loss: 0.0010349351315842375, Optimizer lr: 0.01\n",
      "Epoch: 129, Training Loss: 0.0010292357850325276, Optimizer lr: 0.01\n",
      "Epoch: 130, Training Loss: 0.0010234650193408322, Optimizer lr: 0.01\n",
      "Epoch: 131, Training Loss: 0.0010177311923291407, Optimizer lr: 0.01\n",
      "Epoch: 132, Training Loss: 0.0010120172953247937, Optimizer lr: 0.01\n",
      "Epoch: 133, Training Loss: 0.0010064588697484534, Optimizer lr: 0.01\n",
      "Epoch: 134, Training Loss: 0.0010009251182278307, Optimizer lr: 0.01\n",
      "Epoch: 135, Training Loss: 0.000995366954053621, Optimizer lr: 0.01\n",
      "Epoch: 136, Training Loss: 0.0009898096386680997, Optimizer lr: 0.01\n",
      "Epoch: 137, Training Loss: 0.0009842659628427337, Optimizer lr: 0.01\n",
      "Epoch: 138, Training Loss: 0.0009787706276781197, Optimizer lr: 0.01\n",
      "Epoch: 139, Training Loss: 0.00097345390165441, Optimizer lr: 0.01\n",
      "Epoch: 140, Training Loss: 0.0009682291721679448, Optimizer lr: 0.01\n",
      "Epoch: 141, Training Loss: 0.0009626717603825197, Optimizer lr: 0.01\n",
      "Epoch: 142, Training Loss: 0.0009572356131005345, Optimizer lr: 0.01\n",
      "Epoch: 143, Training Loss: 0.0009519152757924543, Optimizer lr: 0.01\n",
      "Epoch: 144, Training Loss: 0.0009466393901592071, Optimizer lr: 0.01\n",
      "Epoch: 145, Training Loss: 0.0009413417564961382, Optimizer lr: 0.01\n",
      "Epoch: 146, Training Loss: 0.0009360952327315637, Optimizer lr: 0.01\n",
      "Epoch: 147, Training Loss: 0.000930891113160516, Optimizer lr: 0.01\n",
      "Epoch: 148, Training Loss: 0.0009258220477508478, Optimizer lr: 0.01\n",
      "Epoch: 149, Training Loss: 0.0009206266107088816, Optimizer lr: 0.01\n",
      "Epoch: 150, Training Loss: 0.0009155036518002913, Optimizer lr: 0.01\n",
      "Epoch: 151, Training Loss: 0.0009105841051457145, Optimizer lr: 0.01\n",
      "Epoch: 152, Training Loss: 0.0009056380266012557, Optimizer lr: 0.01\n",
      "Epoch: 153, Training Loss: 0.00090046241838839, Optimizer lr: 0.01\n",
      "Epoch: 154, Training Loss: 0.0008954456202403812, Optimizer lr: 0.01\n",
      "Epoch: 155, Training Loss: 0.0008904790992727344, Optimizer lr: 0.01\n",
      "Epoch: 156, Training Loss: 0.0008855448053431612, Optimizer lr: 0.01\n",
      "Epoch: 157, Training Loss: 0.0008807565441343228, Optimizer lr: 0.01\n",
      "Epoch: 158, Training Loss: 0.0008760652228924693, Optimizer lr: 0.01\n",
      "Epoch: 159, Training Loss: 0.0008710579264188137, Optimizer lr: 0.01\n",
      "Epoch: 160, Training Loss: 0.0008661624703425924, Optimizer lr: 0.01\n",
      "Epoch: 161, Training Loss: 0.0008613754980876759, Optimizer lr: 0.01\n",
      "Epoch: 162, Training Loss: 0.0008567379828975323, Optimizer lr: 0.01\n",
      "Epoch: 163, Training Loss: 0.0008519328933441697, Optimizer lr: 0.01\n",
      "Epoch: 164, Training Loss: 0.0008472209541171902, Optimizer lr: 0.01\n",
      "Epoch: 165, Training Loss: 0.0008424985329692082, Optimizer lr: 0.01\n",
      "Epoch: 166, Training Loss: 0.0008378848907648042, Optimizer lr: 0.01\n",
      "Epoch: 167, Training Loss: 0.0008333836617422256, Optimizer lr: 0.01\n",
      "Epoch: 168, Training Loss: 0.0008286760179849888, Optimizer lr: 0.01\n",
      "Epoch: 169, Training Loss: 0.0008240685180398477, Optimizer lr: 0.01\n",
      "Epoch: 170, Training Loss: 0.0008195489292747682, Optimizer lr: 0.01\n",
      "Epoch: 171, Training Loss: 0.0008150815153920061, Optimizer lr: 0.01\n",
      "Epoch: 172, Training Loss: 0.0008106161090925818, Optimizer lr: 0.01\n",
      "Epoch: 173, Training Loss: 0.0008061113838508441, Optimizer lr: 0.01\n",
      "Epoch: 174, Training Loss: 0.0008016714779257648, Optimizer lr: 0.01\n",
      "Epoch: 175, Training Loss: 0.0007972296804835132, Optimizer lr: 0.01\n",
      "Epoch: 176, Training Loss: 0.0007929420793484091, Optimizer lr: 0.01\n",
      "Epoch: 177, Training Loss: 0.0007887838716765244, Optimizer lr: 0.01\n",
      "Epoch: 178, Training Loss: 0.0007843090061756394, Optimizer lr: 0.01\n",
      "Epoch: 179, Training Loss: 0.0007799323433808046, Optimizer lr: 0.01\n",
      "Epoch: 180, Training Loss: 0.000775626741132499, Optimizer lr: 0.01\n",
      "Epoch: 181, Training Loss: 0.0007714808587266951, Optimizer lr: 0.01\n",
      "Epoch: 182, Training Loss: 0.0007671494590045767, Optimizer lr: 0.01\n",
      "Epoch: 183, Training Loss: 0.000762963447539356, Optimizer lr: 0.01\n",
      "Epoch: 184, Training Loss: 0.0007587671990581376, Optimizer lr: 0.01\n",
      "Epoch: 185, Training Loss: 0.0007545890046359216, Optimizer lr: 0.01\n",
      "Epoch: 186, Training Loss: 0.0007505305435117201, Optimizer lr: 0.01\n",
      "Epoch: 187, Training Loss: 0.000746389337989584, Optimizer lr: 0.01\n",
      "Epoch: 188, Training Loss: 0.0007422775186539491, Optimizer lr: 0.01\n",
      "Epoch: 189, Training Loss: 0.0007382910008987009, Optimizer lr: 0.01\n",
      "Epoch: 190, Training Loss: 0.0007342615859406024, Optimizer lr: 0.01\n",
      "Epoch: 191, Training Loss: 0.0007302745340984387, Optimizer lr: 0.01\n",
      "Epoch: 192, Training Loss: 0.0007262114602993619, Optimizer lr: 0.01\n",
      "Epoch: 193, Training Loss: 0.0007222385478887619, Optimizer lr: 0.01\n",
      "Epoch: 194, Training Loss: 0.000718290488167379, Optimizer lr: 0.01\n",
      "Epoch: 195, Training Loss: 0.0007143496155692623, Optimizer lr: 0.01\n",
      "Epoch: 196, Training Loss: 0.0007105620518903015, Optimizer lr: 0.01\n",
      "Epoch: 197, Training Loss: 0.0007066371587785608, Optimizer lr: 0.01\n",
      "Epoch: 198, Training Loss: 0.0007027431585806915, Optimizer lr: 0.01\n",
      "Epoch: 199, Training Loss: 0.0006989085707860365, Optimizer lr: 0.01\n",
      "Epoch: 200, Training Loss: 0.0006952211074829884, Optimizer lr: 0.01\n",
      "Epoch: 201, Training Loss: 0.0006913799161066298, Optimizer lr: 0.01\n",
      "Epoch: 202, Training Loss: 0.0006876294268123307, Optimizer lr: 0.01\n",
      "Epoch: 203, Training Loss: 0.0006838241482936791, Optimizer lr: 0.01\n",
      "Epoch: 204, Training Loss: 0.0006800960640581436, Optimizer lr: 0.01\n",
      "Epoch: 205, Training Loss: 0.0006764051509701768, Optimizer lr: 0.01\n",
      "Epoch: 206, Training Loss: 0.0006727140583290222, Optimizer lr: 0.01\n",
      "Epoch: 207, Training Loss: 0.0006691094695945633, Optimizer lr: 0.01\n",
      "Epoch: 208, Training Loss: 0.0006654585542344585, Optimizer lr: 0.01\n",
      "Epoch: 209, Training Loss: 0.0006618126876355623, Optimizer lr: 0.01\n",
      "Epoch: 210, Training Loss: 0.0006581958018959048, Optimizer lr: 0.01\n",
      "Epoch: 211, Training Loss: 0.0006548750183158278, Optimizer lr: 0.01\n",
      "Epoch: 212, Training Loss: 0.00065122191900284, Optimizer lr: 0.01\n",
      "Epoch: 213, Training Loss: 0.0006475993595772179, Optimizer lr: 0.01\n",
      "Epoch: 214, Training Loss: 0.0006439136201689103, Optimizer lr: 0.01\n",
      "Epoch: 215, Training Loss: 0.0006403333836729704, Optimizer lr: 0.01\n",
      "Epoch: 216, Training Loss: 0.0006367861807932917, Optimizer lr: 0.01\n",
      "Epoch: 217, Training Loss: 0.0006333235465518708, Optimizer lr: 0.01\n",
      "Epoch: 218, Training Loss: 0.0006301641489447322, Optimizer lr: 0.01\n",
      "Epoch: 219, Training Loss: 0.0006264113481899301, Optimizer lr: 0.01\n",
      "Epoch: 220, Training Loss: 0.0006229461744524366, Optimizer lr: 0.01\n",
      "Epoch: 221, Training Loss: 0.000619790574679781, Optimizer lr: 0.01\n",
      "Epoch: 222, Training Loss: 0.0006162901348255447, Optimizer lr: 0.01\n",
      "Epoch: 223, Training Loss: 0.0006128342065358061, Optimizer lr: 0.01\n",
      "Epoch: 224, Training Loss: 0.0006094703490266382, Optimizer lr: 0.01\n",
      "Epoch: 225, Training Loss: 0.0006061553171140391, Optimizer lr: 0.01\n",
      "Epoch: 226, Training Loss: 0.0006028053669334547, Optimizer lr: 0.01\n",
      "Epoch: 227, Training Loss: 0.0005995548330696248, Optimizer lr: 0.01\n",
      "Epoch: 228, Training Loss: 0.0005964566385947116, Optimizer lr: 0.01\n",
      "Epoch: 229, Training Loss: 0.0005930165387687115, Optimizer lr: 0.01\n",
      "Epoch: 230, Training Loss: 0.0005898326452758793, Optimizer lr: 0.01\n",
      "Epoch: 231, Training Loss: 0.0005865599522087028, Optimizer lr: 0.01\n",
      "Epoch: 232, Training Loss: 0.0005833531199350648, Optimizer lr: 0.01\n",
      "Epoch: 233, Training Loss: 0.0005802061150061767, Optimizer lr: 0.01\n",
      "Epoch: 234, Training Loss: 0.0005772770945195262, Optimizer lr: 0.01\n",
      "Epoch: 235, Training Loss: 0.0005740357175816364, Optimizer lr: 0.01\n",
      "Epoch: 236, Training Loss: 0.0005708857124822258, Optimizer lr: 0.01\n",
      "Epoch: 237, Training Loss: 0.0005677241309525124, Optimizer lr: 0.01\n",
      "Epoch: 238, Training Loss: 0.0005646458741331489, Optimizer lr: 0.01\n",
      "Epoch: 239, Training Loss: 0.0005615827822063307, Optimizer lr: 0.01\n",
      "Epoch: 240, Training Loss: 0.0005585172757662238, Optimizer lr: 0.01\n",
      "Epoch: 241, Training Loss: 0.0005555046830994863, Optimizer lr: 0.01\n",
      "Epoch: 242, Training Loss: 0.000552600436081848, Optimizer lr: 0.01\n",
      "Epoch: 243, Training Loss: 0.0005497353770089649, Optimizer lr: 0.01\n",
      "Epoch: 244, Training Loss: 0.000546606933710629, Optimizer lr: 0.01\n",
      "Epoch: 245, Training Loss: 0.0005436117886762824, Optimizer lr: 0.01\n",
      "Epoch: 246, Training Loss: 0.0005406750512128019, Optimizer lr: 0.01\n",
      "Epoch: 247, Training Loss: 0.0005377631965096342, Optimizer lr: 0.01\n",
      "Epoch: 248, Training Loss: 0.0005348648755912029, Optimizer lr: 0.01\n",
      "Epoch: 249, Training Loss: 0.0005320566242681584, Optimizer lr: 0.01\n",
      "Epoch: 250, Training Loss: 0.0005291381814253216, Optimizer lr: 0.01\n",
      "Epoch: 251, Training Loss: 0.0005264826461762232, Optimizer lr: 0.01\n",
      "Epoch: 252, Training Loss: 0.0005235049113530469, Optimizer lr: 0.01\n",
      "Epoch: 253, Training Loss: 0.0005206322422108625, Optimizer lr: 0.01\n",
      "Epoch: 254, Training Loss: 0.0005178496427109368, Optimizer lr: 0.01\n",
      "Epoch: 255, Training Loss: 0.0005150621961367926, Optimizer lr: 0.01\n",
      "Epoch: 256, Training Loss: 0.0005123536254735065, Optimizer lr: 0.01\n",
      "Epoch: 257, Training Loss: 0.0005095782067648836, Optimizer lr: 0.01\n",
      "Epoch: 258, Training Loss: 0.0005068136059740917, Optimizer lr: 0.01\n",
      "Epoch: 259, Training Loss: 0.0005041679656930679, Optimizer lr: 0.01\n",
      "Epoch: 260, Training Loss: 0.0005015689358993528, Optimizer lr: 0.01\n",
      "Epoch: 261, Training Loss: 0.0004987994794275338, Optimizer lr: 0.01\n",
      "Epoch: 262, Training Loss: 0.0004960846618213556, Optimizer lr: 0.01\n",
      "Epoch: 263, Training Loss: 0.0004934749413916762, Optimizer lr: 0.01\n",
      "Epoch: 264, Training Loss: 0.000490794623612074, Optimizer lr: 0.01\n",
      "Epoch: 265, Training Loss: 0.00048816115921294104, Optimizer lr: 0.01\n",
      "Epoch: 266, Training Loss: 0.00048555142727663397, Optimizer lr: 0.01\n",
      "Epoch: 267, Training Loss: 0.0004829491097558586, Optimizer lr: 0.01\n",
      "Epoch: 268, Training Loss: 0.00048046570741277204, Optimizer lr: 0.01\n",
      "Epoch: 269, Training Loss: 0.0004779820509276968, Optimizer lr: 0.01\n",
      "Epoch: 270, Training Loss: 0.0004753177526528073, Optimizer lr: 0.01\n",
      "Epoch: 271, Training Loss: 0.00047275254049619175, Optimizer lr: 0.01\n",
      "Epoch: 272, Training Loss: 0.00047027481346727023, Optimizer lr: 0.01\n",
      "Epoch: 273, Training Loss: 0.00046772991550171616, Optimizer lr: 0.01\n",
      "Epoch: 274, Training Loss: 0.0004652439221145259, Optimizer lr: 0.01\n",
      "Epoch: 275, Training Loss: 0.00046275514353569984, Optimizer lr: 0.01\n",
      "Epoch: 276, Training Loss: 0.0004603034684894741, Optimizer lr: 0.01\n",
      "Epoch: 277, Training Loss: 0.00045784290048345546, Optimizer lr: 0.01\n",
      "Epoch: 278, Training Loss: 0.0004554971747964448, Optimizer lr: 0.01\n",
      "Epoch: 279, Training Loss: 0.00045315416572436727, Optimizer lr: 0.01\n",
      "Epoch: 280, Training Loss: 0.0004506517044683427, Optimizer lr: 0.01\n",
      "Epoch: 281, Training Loss: 0.00044822109850781297, Optimizer lr: 0.01\n",
      "Epoch: 282, Training Loss: 0.0004458511089787301, Optimizer lr: 0.01\n",
      "Epoch: 283, Training Loss: 0.00044347679442256955, Optimizer lr: 0.01\n",
      "Epoch: 284, Training Loss: 0.0004411387276349717, Optimizer lr: 0.01\n",
      "Epoch: 285, Training Loss: 0.0004388217262485043, Optimizer lr: 0.01\n",
      "Epoch: 286, Training Loss: 0.00043656163317088966, Optimizer lr: 0.01\n",
      "Epoch: 287, Training Loss: 0.00043419206321059124, Optimizer lr: 0.01\n",
      "Epoch: 288, Training Loss: 0.00043188729292392757, Optimizer lr: 0.01\n",
      "Epoch: 289, Training Loss: 0.00042959445706716834, Optimizer lr: 0.01\n",
      "Epoch: 290, Training Loss: 0.0004273606181049057, Optimizer lr: 0.01\n",
      "Epoch: 291, Training Loss: 0.0004251980097081324, Optimizer lr: 0.01\n",
      "Epoch: 292, Training Loss: 0.0004228654943009752, Optimizer lr: 0.01\n",
      "Epoch: 293, Training Loss: 0.00042060151638714094, Optimizer lr: 0.01\n",
      "Epoch: 294, Training Loss: 0.000418394548863416, Optimizer lr: 0.01\n",
      "Epoch: 295, Training Loss: 0.00041621329195169206, Optimizer lr: 0.01\n",
      "Epoch: 296, Training Loss: 0.00041399634501272214, Optimizer lr: 0.01\n",
      "Epoch: 297, Training Loss: 0.0004118109360856352, Optimizer lr: 0.01\n",
      "Epoch: 298, Training Loss: 0.0004096392026600371, Optimizer lr: 0.01\n",
      "Epoch: 299, Training Loss: 0.00040748296038519723, Optimizer lr: 0.01\n",
      "Epoch: 300, Training Loss: 0.0004053405158022561, Optimizer lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "model.train(X_in, Y_true, 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
